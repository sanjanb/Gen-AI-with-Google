# Endnotes

1. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I., 2017, [Attention is all you need](https://arxiv.org/abs/1706.03762). Advances in Neural Information Processing Systems, 30.
2. Wikipedia, 2024, [Word n-gram language model](https://en.wikipedia.org/wiki/Word_n-gram_language_model).
3. Sutskever, I., Vinyals, O., & Le, Q. V., 2014, [Sequence to sequence learning with neural networks](https://arxiv.org/abs/1409.3215). Advances in Neural Information Processing Systems, 27.
4. Gu, A., Goel, K., & Ré, C., 2021, [Efficiently modeling long sequences with structured state spaces](https://arxiv.org/abs/2111.00396).
5. Jalammar, J. (n.d.). [The illustrated transformer](https://jalammar.github.io/illustrated-transformer/).
6. Ba, J. L., Kiros, J. R., & Hinton, G. E., 2016, [Layer normalization](https://arxiv.org/abs/1607.06450).
7. He, K., Zhang, X., Ren, S., & Sun, J., 2016, [Deep residual learning for image recognition](https://arxiv.org/abs/1512.03385). Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.
8. HuggingFace., 2024, [Byte Pair Encoding](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt).
9. Kudo, T., & Richardson, J., 2018, [Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing](https://arxiv.org/abs/1808.06226).
10. HuggingFace, 2024, [Unigram tokenization](https://huggingface.co/learn/nlp-course/chapter6/7?fw=pt).
11. Goodfellow et. al., 2016, [Deep Learning](http://www.deeplearningbook.org). MIT Press.
12. Radford, Alec et al., 2019, [Language models are unsupervised multitask learners](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf).
13. Brown, Tom, et al., 2020, [Language models are few-shot learners](https://arxiv.org/abs/2005.14165). Advances in Neural Information Processing Systems, 33, 1877-1901.
14. Devlin, Jacob, et al., 2018, [BERT: Pre-training of deep bidirectional transformers for language understanding](https://arxiv.org/abs/1810.04805).
15. Radford, A., & Narasimhan, K., 2018, [Improving language understanding by generative pre-training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf).
16. Dai, A., & Le, Q., 2015, [Semi-supervised sequence learning](https://arxiv.org/abs/1511.01432). Advances in Neural Information Processing Systems.
17. Ouyang, Long, et al., 2022, [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155). Advances in Neural Information Processing Systems, 35, 27730-27744.
18. OpenAI., 2023, [GPT-3.5](https://platform.openai.com/docs/models/gpt-3-5).
19. OpenAI., 2023, [GPT-4 Technical Report](https://arxiv.org/abs/2303.08774).
20. Thoppilan, Romal, et al., 2022, [Lamda: Language models for dialog applications](https://arxiv.org/abs/2201.08239).
21. [Llama 3.2: Revolutionizing edge AI and vision with open, customizable models](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/).
22. Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., ... & Irving, G., 2021, [Scaling language models: Methods, analysis & insights from training Gopher](https://arxiv.org/pdf/2112.11446.pdf).
23. Du, N., He, H., Dai, Z., Mccarthy, J., Patwary, M. A., & Zhou, L., 2022, [GLAM: Efficient scaling of language models with mixture-of-experts](https://proceedings.mlr.press/v162/du22a.html). In International Conference on Machine Learning (pp. 2790-2800). PMLR.
24. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., ... & Amodei, D., 2020, [Scaling laws for neural language models](https://arxiv.org/abs/2001.08361).
25. Hoffmann, Jordan, et al., 2022, [Training compute-optimal large language models](https://arxiv.org/abs/2203.15556).
26. Shoeybi, Mohammad, et al., 2019, [Megatron-LM: Training multi-billion parameter language models using model parallelism](https://arxiv.org/abs/1909.08053).
27. Muennighoff, N. et al., 2023, [Scaling data-constrained language models](https://arxiv.org/abs/2305.16264).
28. Chowdhery, Aakanksha, et al., 2023, [Palm: Scaling language modeling with pathways](https://arxiv.org/abs/2204.02311). Journal of Machine Learning Research, 24(240), 1-113.
29. Wang, Alex, et al.,2019, [SuperGLUE: A stickier benchmark for general-purpose language understanding systems](https://arxiv.org/abs/1905.00537). Advances in Neural Information Processing Systems, 32.
30. Anil, Rohan, et al., 2023, [Palm 2 technical report](https://arxiv.org/abs/2305.10403).
31. DeepMind, 2023, [Gemini: A family of highly capable multimodal models](https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf).
32. DeepMind, 2024, [Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf).
33. Google Developers, 2024, [Introducing PaLi-Gemma, Gemma 2, and an upgraded responsible AI toolkit](https://developers.googleblog.com/en/gemma-family-and-toolkit-expansion-io-2024/).
34. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M., Lacroix, T., ... & Jegou, H., 2023, [Llama 2: Open foundation and fine-tuned chat models](https://arxiv.org/abs/2307.09288).
35. Jiang, A. Q., 2024, [Mixtral of experts](https://arxiv.org/abs/2401.04088).
36. Qwen, 2024, [Introducing Qwen1.5](https://qwenlm.github.io/blog/qwen1.5/).
37. Young, A., 2024, [Yi: Open foundation models by 01.AI](https://arxiv.org/abs/2403.04652).
38. Grok-1, 2024, [Grok-1 GitHub](https://github.com/xai-org/grok-1).
39. Duan, Haodong, et al., 2023, [BotChat: Evaluating LLMs’ capabilities of having multi-turn dialogues](https://arxiv.org/abs/2310.13650).
40. Google Cloud, 2024, [Tune text models with reinforcement learning from human feedback](https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-text-models-rlhf).
41. Bai, Yuntao, et al., 2022, [Constitutional AI: Harmlessness from AI feedback](https://arxiv.org/abs/2212.08073).
42. Wikipedia, 2024, [Likert scale](https://en.wikipedia.org/wiki/Likert_scale).
43. Sutton, R. S., & Barto, A. G., 2018, [Reinforcement learning: An introduction](http://incompleteideas.net/book/the-book-2nd.html). MIT Press.
44. Bai, Yuntao, et al, 2022, [Constitutional AI: Harmlessness from AI feedback](https://arxiv.org/abs/2212.08073).
45. Rafailov, Rafael, et al., 2023, [Direct preference optimization: Your language model is secretly a reward model](https://arxiv.org/abs/2305.18290).
46. Houlsby, Neil, et al., 2019, [Parameter-efficient transfer learning for NLP](https://proceedings.mlr.press/v97/houlsby19a.html). In International Conference on Machine Learning (pp. 2790-2799). PMLR.
47. Hu, Edward J., et al., 2021, [LoRA: Low-rank adaptation of large language models](https://arxiv.org/abs/2106.09685).
48. Dettmers, Tim, et al., 2023, [QLoRA: Efficient finetuning of quantized LLMs](https://arxiv.org/abs/2305.14314).
49. Lester, B., Al-Rfou, R., & Constant, N., 2021, [The power of scale for parameter-efficient prompt tuning](https://arxiv.org/abs/2104.08691).
50. HuggingFace., 2020, [How to generate text?](https://huggingface.co/blog/how-to-generate).
51. Google AI Studio Context caching. Available at: [Context caching](https://ai.google.dev/gemini-api/docs/caching?lang=python).
52. Vertex AI Context caching overview. Available at: [Context caching overview](https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview).
53. Gu, A., Goel, K., & Ré, C., 2021, [Efficiently modeling long sequences with structured state spaces](https://arxiv.org/abs/2111.00396).
54. Hubara et al., 2016, [Quantized neural networks: Training neural networks with low precision weights and activations](https://arxiv.org/abs/1609.07061).
55. Benoit Jacob et al., 2017, [Quantization and training of neural networks for efficient integer-arithmetic-only inference](https://arxiv.org/abs/1712.05877).
56. Bucila, C., Caruana, R., & Niculescu-Mizil, A., 2006, [Model compression](https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf). Knowledge Discovery and Data Mining.
57. Hinton, G., Vinyals, O., & Dean, J., 2015, [Distilling the knowledge in a neural network](https://arxiv.org/abs/1503.02531).
58. Zhang, L., Fei, W., Wu w., He Y., Lou Z., Zhou H., 2023, [Dual Grained Quantisation: Efficient Finegrained Quantisation for LLM](https://arxiv.org/abs/2310.04836).
59. Agarwal, R., Vieillard, N., Zhou, Y., Stanczyk, P., Ramos, S., Geist, M., Bachem, O., 2024, [On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes](https://arxiv.org/abs/2306.13649).
60. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., & Dean, J., 2017, [Outrageously large neural networks: The sparsely-gated mixture-of-experts layer](https://arxiv.org/abs/1701.06538).
61. Schuster, T., Fried, D., & Jurafsky, D., 2022, [Confident adaptive language modeling](https://arxiv.org/abs/2207.07061).
62. Tri Dao et al. [FlashAttention](https://arxiv.org/abs/2205.14135).
63. Leviathan, Y., Ram, O., Desbordes, T., & Haussmann, E., 2022, [Fast inference from transformers via speculative decoding](https://arxiv.org/abs/2211.17192).
64. Li, Y., Humphreys, P., Sun, T., Carr, A., Cass, S., Hawkins, P., ... & Bortolussi, L., 2022, [Competition-level code generation with AlphaCode](https://www.science.org/doi/10.1126/science.abq1158). Science, 378(1092-1097).
65. Romera-Paredes, B., Barekatain, M., Novikov, A., Novikov, A., Rashed, S., & Yang, J., 2023, [Mathematical discoveries from program search with large language models](https://www.nature.com/articles/s41586-023-06924-6). Nature.
66. Wikipedia., 2024, [Cap set](https://en.wikipedia.org/wiki/Cap_set).
67. Trinh, T. H., Wu, Y., & Le, Q. V. et al., 2024, [Solving olympiad geometry without human demonstrations](https://www.nature.com/articles/s41586-023-06747-5). Nature, 625, 476–482.
68. Mikolov, T., Chen, K., Corrado, G., Dean, J., 2013, [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781).
69. Shi, L., Ma, C., Liang, W., Ma, W., Vosoughi, S., 2024, [Judging the Judges: A Systematic Study of Position Bias in LLM-as-a-Judge](https://arxiv.org/abs/2406.07791).
70. Pandit, B., 2024, [What Is Mixture of Experts (MoE)? How It Works, Use Cases & More](https://www.datacamp.com/blog/mixture-of-experts-moe).
