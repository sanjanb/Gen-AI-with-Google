# References

1. Shafran, I., Cao, Y. et al., 2022, 'ReAct: Synergizing Reasoning and Acting in Language Models'. Available at: [arXiv](https://arxiv.org/abs/2210.03629)
2. Wei, J., Wang, X. et al., 2023, 'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models'. Available at: [arXiv](https://arxiv.org/pdf/2201.11903.pdf)
3. Wang, X. et al., 2022, 'Self-Consistency Improves Chain of Thought Reasoning in Language Models'. Available at: [arXiv](https://arxiv.org/abs/2203.11171)
4. Diao, S. et al., 2023, 'Active Prompting with Chain-of-Thought for Large Language Models'. Available at: [arXiv](https://arxiv.org/pdf/2302.12246.pdf)
5. Zhang, H. et al., 2023, 'Multimodal Chain-of-Thought Reasoning in Language Models'. Available at: [arXiv](https://arxiv.org/abs/2302.00923)
6. Yao, S. et al., 2023, 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models'. Available at: [arXiv](https://arxiv.org/abs/2305.10601)
7. Long, X., 2023, 'Large Language Model Guided Tree-of-Thought'. Available at: [arXiv](https://arxiv.org/abs/2305.08291)
8. Google. 'Google Gemini Application'. Available at: [Google Gemini](http://gemini.google.com)
9. Swagger. 'OpenAPI Specification'. Available at: [Swagger](https://swagger.io/specification/)
10. Xie, M., 2022, 'How does in-context learning work? A framework for understanding the differences from traditional supervised learning'. Available at: [Stanford AI](https://ai.stanford.edu/blog/understanding-incontext/)
11. Google Research. 'ScaNN (Scalable Nearest Neighbors)'. Available at: [GitHub](https://github.com/google-research/google-research/tree/master/scann)
12. LangChain. 'LangChain'. Available at: [LangChain](https://python.langchain.com/v0.2/docs/introduction/)
